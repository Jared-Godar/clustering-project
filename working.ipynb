{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdb6fb9",
   "metadata": {},
   "source": [
    "# Working notebook for clustering project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "\n",
    "#Vizualization Tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Modeling Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Custim functions\n",
    "from env import host, user, password #Database credentials\n",
    "import wrangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL from ENV\n",
    "def get_db_url(database):\n",
    "    '''\n",
    "    Gets appropriate url to pull data from credentials stored in env file\n",
    "    '''\n",
    "    from env import host, user, password\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "\n",
    "def get_zillow():\n",
    "    query = '''\n",
    "    SELECT prop.*, \n",
    "        pred.logerror, \n",
    "        pred.transactiondate, \n",
    "        air.airconditioningdesc, \n",
    "        arch.architecturalstyledesc, \n",
    "        build.buildingclassdesc, \n",
    "        heat.heatingorsystemdesc, \n",
    "        landuse.propertylandusedesc, \n",
    "        story.storydesc, \n",
    "        construct.typeconstructiondesc \n",
    "\n",
    "    FROM   properties_2017 prop  \n",
    "        INNER JOIN (SELECT parcelid,\n",
    "                            logerror,\n",
    "                            Max(transactiondate) transactiondate \n",
    "                    FROM   predictions_2017 \n",
    "                    GROUP  BY parcelid, logerror) pred\n",
    "                USING (parcelid) \n",
    "        LEFT JOIN airconditioningtype air USING (airconditioningtypeid) \n",
    "        LEFT JOIN architecturalstyletype arch USING (architecturalstyletypeid) \n",
    "        LEFT JOIN buildingclasstype build USING (buildingclasstypeid) \n",
    "        LEFT JOIN heatingorsystemtype heat USING (heatingorsystemtypeid) \n",
    "        LEFT JOIN propertylandusetype landuse USING (propertylandusetypeid) \n",
    "        LEFT JOIN storytype story USING (storytypeid) \n",
    "        LEFT JOIN typeconstructiontype construct USING (typeconstructiontypeid) \n",
    "    WHERE  prop.latitude IS NOT NULL \n",
    "        AND prop.longitude IS NOT NULL AND transactiondate <= '2017-12-31' \n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql(query, get_db_url('zillow'), index_col='id')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull values from SQL\n",
    "\n",
    "#df = get_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c75293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write dataframe to csv\n",
    "#df.to_csv('unedited_zillow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "df =pd.read_csv('unedited_zillow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1 = df.shape\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure single use homes\n",
    "\n",
    "def single_use(df):\n",
    "    '''\n",
    "    Ensures we are only looking at single use properties with at least one bedroom and >= 350 sf.\n",
    "    '''\n",
    "    single_use = [261, 262, 263, 264, 266, 268, 273, 276, 279]\n",
    "    df = df[df.propertylandusetypeid.isin(single_use)]\n",
    "    df = df[(df.bedroomcnt > 0) & (df.bathroomcnt > 0) & ((df.unitcnt<=1)|df.unitcnt.isnull())\\\n",
    "            & (df.calculatedfinishedsquarefeet>350)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=single_use(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape2 = df.shape\n",
    "print(shape1)\n",
    "print(shape2)\n",
    "print(f'Dropped {shape1[0]-shape2[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_county(df):\n",
    "    '''\n",
    "    Add column for counties\n",
    "    '''\n",
    "    import numpy as np\n",
    "    df['county'] = np.where(df.fips == 6037, 'Los_Angeles',\n",
    "                           np.where(df.fips == 6059, 'Orange', \n",
    "                                   'Ventura'))    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_county(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c98bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_column = .5, prop_required_row = .70):\n",
    "    threshold = int(round(prop_required_column*len(df.index),0))\n",
    "    df.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "    threshold = int(round(prop_required_row*len(df.columns),0))\n",
    "    df.dropna(axis=0, thresh=threshold, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb321559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ae9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape3=df.shape\n",
    "print(shape3)\n",
    "print(f'Dropped {shape2[0]-shape3[0]} rows')\n",
    "print(f'Dropped {shape2[1]-shape3[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df, cols_to_remove):  \n",
    "    '''\n",
    "    Pass a list od columns to remove\n",
    "    '''\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):    \n",
    "    # replace nulls with median values for select columns\n",
    "    df.lotsizesquarefeet.fillna(7313, inplace = True)\n",
    "    # Columns to look for outliers\n",
    "    df = df[df.taxvaluedollarcnt < 5_000_000]\n",
    "    df[df.calculatedfinishedsquarefeet < 8000]\n",
    "    # Just to be sure we caught all nulls, drop them here\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape4=df.shape\n",
    "print(shape4)\n",
    "print(f'Dropped {shape3[0]-shape4[0]} rows')\n",
    "print(f'Dropped {shape3[1]-shape4[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, col_list, k=1.5):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        q1, q3 = df[col].quantile([.25, .75])  # get quartiles\n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "        # return dataframe without outliers\n",
    "        df = df[(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279353e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'fullbathcnt', 'lotsizesquarefeet', 'roomcnt', 'unitcnt','structuretaxvaluedollarcnt', 'taxvaluedollarcnt','taxamount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4660",
   "metadata": {},
   "source": [
    "- [ ] Throw out outliers in target?\n",
    "- [ ] What about year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = remove_outliers(df, ['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'fullbathcnt', 'lotsizesquarefeet', 'roomcnt', 'unitcnt','structuretaxvaluedollarcnt', 'taxvaluedollarcnt','taxamount'], 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c28e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape5=df.shape\n",
    "print(shape5)\n",
    "print(f'Dropped {shape4[0]-shape5[0]} rows')\n",
    "print(f'Dropped {shape4[1]-shape5[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf14b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_my_data(df, pct=0.10):\n",
    "    '''\n",
    "    This splits a dataframe into train, validate, and test sets. \n",
    "    df = dataframe to split\n",
    "    pct = size of the test set, 1/2 of size of the validate set\n",
    "    Returns three dataframes (train, validate, test)\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=pct, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size=pct*2, random_state = 123)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a76dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_my_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c299dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(train, validate, test):\n",
    "    '''\n",
    "    Uses the train & test datasets created by the split_my_data function\n",
    "    Returns 3 items: mm_scaler, train_scaled_mm, test_scaled_mm\n",
    "    This is a linear transformation. Values will lie between 0 and 1\n",
    "    '''\n",
    "    num_vars = list(train.select_dtypes('number').columns)\n",
    "    scaler = MinMaxScaler(copy=True, feature_range=(0,1))\n",
    "    train[num_vars] = scaler.fit_transform(train[num_vars])\n",
    "    validate[num_vars] = scaler.transform(validate[num_vars])\n",
    "    test[num_vars] = scaler.transform(test[num_vars])\n",
    "    return scaler, train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_baseline(train, validate, test):\n",
    "    '''\n",
    "    Assigns mean error as baseline prediction\n",
    "    '''\n",
    "    baseline = train.logerror.mean()\n",
    "    train['baseline'] = baseline\n",
    "    validate['baseline'] = baseline\n",
    "    test['baseline'] = baseline\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = add_baseline(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ab104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbaca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(train, validate, test):\n",
    "    '''\n",
    "    Splits dataframe into train, validate, and test data frames\n",
    "    '''\n",
    "    X_train = train.drop(columns='logerror')\n",
    "    y_train = train.logerror\n",
    "\n",
    "    X_validate = validate.drop(columns='logerror')\n",
    "    y_validate = validate.logerror\n",
    "\n",
    "    X_test = test.drop(columns='logerror')\n",
    "    y_test = test.logerror\n",
    "\n",
    "    return train, X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, X_train, y_train, X_validate, y_validate, X_test, y_test = split_xy(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_validate, X_test, train, validate, test):\n",
    "    '''\n",
    "    Uses the train & test datasets created by the split_my_data function\n",
    "    Returns 3 items: mm_scaler, train_scaled_mm, test_scaled_mm\n",
    "    This is a linear transformation. Values will lie between 0 and 1\n",
    "    '''\n",
    "    num_vars = list(X_train.select_dtypes('number').columns)\n",
    "    scaler = MinMaxScaler(copy=True, feature_range=(0,1))\n",
    "    train[num_vars] = scaler.fit_transform(X_train[num_vars])\n",
    "    validate[num_vars] = scaler.transform(X_validate[num_vars])\n",
    "    test[num_vars] = scaler.transform(X_test[num_vars])\n",
    "    return X_train, X_validate, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, X_test = scale(X_train, X_validate, X_test, train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrangle function\n",
    "def wrangle():\n",
    "    #df = get_zillow()\n",
    "    #df.to_csv('unedited_zillow.csv')\n",
    "    df =pd.read_csv('unedited_zillow.csv')\n",
    "    df = single_use(df)\n",
    "    df = add_county(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = clean(df)\n",
    "    #columns=['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'fullbathcnt', 'lotsizesquarefeet', 'roomcnt', 'unitcnt','structuretaxvaluedollarcnt', 'taxvaluedollarcnt','taxamount']\n",
    "    #df = remove_outliers(df, columns)\n",
    "    train, validate, test = split_my_data(df)\n",
    "    train, validate, test = add_baseline(train, validate, test)\n",
    "    train, X_train, y_train, X_validate, y_validate, X_test, y_test = split_xy(train, validate, test)\n",
    "    X_train, X_validate, X_test = scale(X_train, X_validate, X_test, train, validate, test)\n",
    "    return train, X_train, y_train, X_validate, y_validate, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, X_train, y_train, X_validate, y_validate, X_test, y_test= wrangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d10dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62d53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
